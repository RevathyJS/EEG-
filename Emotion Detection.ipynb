{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2132 entries, 0 to 2131\n",
      "Columns: 2549 entries, # mean_0_a to label\n",
      "dtypes: float64(2548), object(1)\n",
      "memory usage: 41.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c7f8cf3668>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='label',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# mean_0_a    0\n",
       "mean_1_a      0\n",
       "mean_2_a      0\n",
       "mean_3_a      0\n",
       "mean_4_a      0\n",
       "             ..\n",
       "fft_746_b     0\n",
       "fft_747_b     0\n",
       "fft_748_b     0\n",
       "fft_749_b     0\n",
       "label         0\n",
       "Length: 2549, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
      "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
      "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
      "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
      "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
      "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
      "\n",
      "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
      "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
      "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
      "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
      "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
      "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
      "\n",
      "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
      "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
      "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
      "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
      "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
      "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
      "\n",
      "[5 rows x 2549 columns]\n",
      "0    716\n",
      "1    708\n",
      "2    708\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "encode = ({'NEUTRAL':0, 'POSITIVE':1,'NEGATIVE':2})\n",
    "df_encoded = df.replace(encode)\n",
    "\n",
    "print(df_encoded.head())\n",
    "print(df_encoded['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132, 2548)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df_encoded.drop([\"label\"]  ,axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_encoded.loc[:,'label'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 64)             668928    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 681,443\n",
      "Trainable params: 681,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,2548),activation=\"relu\",return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(LSTM(100,return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(50))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.1754 - val_accuracy: 0.9625\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0119 - accuracy: 0.9953 - val_loss: 0.1355 - val_accuracy: 0.9696\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.1826 - val_accuracy: 0.9625\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - 1s 14ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1827 - val_accuracy: 0.9672\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1766 - val_accuracy: 0.9602\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.1314 - val_accuracy: 0.9742\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9649\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.1488 - val_accuracy: 0.9672\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0138 - accuracy: 0.9971 - val_loss: 0.2339 - val_accuracy: 0.9555\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.1949 - val_accuracy: 0.9602\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2113 - val_accuracy: 0.9602\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2228 - val_accuracy: 0.9578\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0058 - accuracy: 0.9977 - val_loss: 0.1668 - val_accuracy: 0.9649\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1615 - val_accuracy: 0.9696\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.2064 - val_accuracy: 0.9625\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.1613 - val_accuracy: 0.9719\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.1429 - val_accuracy: 0.9719\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.1637 - val_accuracy: 0.9696\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0081 - accuracy: 0.9965 - val_loss: 0.1781 - val_accuracy: 0.9672\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0150 - accuracy: 0.9977 - val_loss: 0.1968 - val_accuracy: 0.9672\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0039 - accuracy: 0.9982 - val_loss: 0.2661 - val_accuracy: 0.9555\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0143 - accuracy: 0.9971 - val_loss: 0.2693 - val_accuracy: 0.9532\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.1940 - val_accuracy: 0.9649\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.1783 - val_accuracy: 0.9602\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2077 - val_accuracy: 0.9508\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.1695 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.1958 - val_accuracy: 0.9578\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.1542 - val_accuracy: 0.9719\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.1580 - val_accuracy: 0.9719\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 7.9624e-04 - accuracy: 1.0000 - val_loss: 0.1641 - val_accuracy: 0.9696\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1525 - val_accuracy: 0.9742\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0069 - accuracy: 0.9994 - val_loss: 0.2163 - val_accuracy: 0.9602\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.2123 - val_accuracy: 0.9578\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.1892 - val_accuracy: 0.9625\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.2843 - val_accuracy: 0.9485\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0049 - accuracy: 0.9994 - val_loss: 0.1731 - val_accuracy: 0.9625\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.2113 - val_accuracy: 0.9555\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.1782 - val_accuracy: 0.9625\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0024 - accuracy: 0.9988 - val_loss: 0.1874 - val_accuracy: 0.9672\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.1721 - val_accuracy: 0.9696\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.1740 - val_accuracy: 0.9696\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.1747 - val_accuracy: 0.9696\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 6.7651e-04 - accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9696\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0045 - accuracy: 0.9977 - val_loss: 0.1905 - val_accuracy: 0.9672\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.1935 - val_accuracy: 0.9649\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1864 - val_accuracy: 0.9672\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9649\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.3160 - val_accuracy: 0.9485\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0121 - accuracy: 0.9977 - val_loss: 0.2381 - val_accuracy: 0.9625\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.2451 - val_accuracy: 0.9555\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0147 - accuracy: 0.9965 - val_loss: 0.2582 - val_accuracy: 0.9555\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0119 - accuracy: 0.9977 - val_loss: 0.2987 - val_accuracy: 0.9508\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0196 - accuracy: 0.9953 - val_loss: 0.2876 - val_accuracy: 0.9485\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.1814 - val_accuracy: 0.9649\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.1922 - val_accuracy: 0.9625\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9719\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 9.5476e-04 - accuracy: 1.0000 - val_loss: 0.1650 - val_accuracy: 0.9696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.1699 - val_accuracy: 0.9696\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.1516 - val_accuracy: 0.9719\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.1519 - val_accuracy: 0.9696\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.1792 - val_accuracy: 0.9649\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.1916 - val_accuracy: 0.9578\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0058 - accuracy: 0.9977 - val_loss: 0.1362 - val_accuracy: 0.9696\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0082 - accuracy: 0.9982 - val_loss: 0.1868 - val_accuracy: 0.9625\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1636 - val_accuracy: 0.9672\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 7.6341e-04 - accuracy: 1.0000 - val_loss: 0.1760 - val_accuracy: 0.9649\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0067 - accuracy: 0.9988 - val_loss: 0.1557 - val_accuracy: 0.9719\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 0.9649\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0154 - accuracy: 0.9959 - val_loss: 0.1779 - val_accuracy: 0.9625\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.2054 - val_accuracy: 0.9625\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0064 - accuracy: 0.9971 - val_loss: 0.1792 - val_accuracy: 0.9696\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.1634 - val_accuracy: 0.9789\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.2050 - val_accuracy: 0.9672\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.1850 - val_accuracy: 0.9696\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.1702 - val_accuracy: 0.9672\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0086 - accuracy: 0.9982 - val_loss: 0.1734 - val_accuracy: 0.9696\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0109 - accuracy: 0.9982 - val_loss: 0.1831 - val_accuracy: 0.9672\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1929 - val_accuracy: 0.9696\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.1689 - val_accuracy: 0.9719\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.1986 - val_accuracy: 0.9719\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.2631 - val_accuracy: 0.9602\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0078 - accuracy: 0.9982 - val_loss: 0.2339 - val_accuracy: 0.9578\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0062 - accuracy: 0.9977 - val_loss: 0.1910 - val_accuracy: 0.9672\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.1673 - val_accuracy: 0.9649\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.1624 - val_accuracy: 0.9696\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 7.5626e-04 - accuracy: 1.0000 - val_loss: 0.1736 - val_accuracy: 0.9672\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0104 - accuracy: 0.9977 - val_loss: 0.1819 - val_accuracy: 0.9602\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.1636 - val_accuracy: 0.9719\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.1589 - val_accuracy: 0.9672\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.1070 - val_accuracy: 0.9766\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.1340 - val_accuracy: 0.9696\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.1454 - val_accuracy: 0.9719\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 8.3747e-04 - accuracy: 1.0000 - val_loss: 0.1441 - val_accuracy: 0.9719\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.1248 - val_accuracy: 0.9742\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.1281 - val_accuracy: 0.9766\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0123 - accuracy: 0.9977 - val_loss: 0.1782 - val_accuracy: 0.9625\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0178 - accuracy: 0.9965 - val_loss: 0.1588 - val_accuracy: 0.9672\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.1548 - val_accuracy: 0.9696\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.1537 - val_accuracy: 0.9719\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.1586 - val_accuracy: 0.9742\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1586 - accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427,)\n",
      "(427,)\n",
      "Training Accuracy: 0.9484777517564403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "print(expected_classes.shape)\n",
    "print(predict_classes.shape)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Training Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
